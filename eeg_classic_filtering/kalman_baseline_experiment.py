import argparse
import typing as tp
import numpy as np
from filterpy.kalman import KalmanFilter
from optuna.study import Study
from tqdm import tqdm
from eeg_classic_filtering.datasets import NPWrapperDataset
from eeg_classic_filtering.helpful_functions import calculate_metrics, save_results
from eeg_classic_filtering.hyperparameters_optimization import optimize_and_test_real, optimize_and_test_synthetic
from general_utilities.utils import make_deterministic


def init_kalman(kalman_A: float, kalman_frequency: float, kalman_sigma: float, kalman_delta: float,
                sampling_rate: float):
    """
    Initializes a new instance of the Kalman filter with chosen parameters

    Parameters
    ----------
    kalman_A : float
                Alpha in the state transition matrix which makes process stationary. Should be in (0, 1) range.
                Typically, is close to 1
    kalman_frequency : float
                Central frequency for the state-space process
    kalman_sigma : float
                Standard deviation for the process noise matrix
    kalman_delta : float
                Standard deviation for the measurement noise matrix
    sampling_rate : float
                Sampling rate of the data to be filtered

    Returns
    -------
    kalman : KalmanFilter
                An initialized instance of the Kalman Filter from filterpy
    """
    kalman = KalmanFilter(dim_x=2, dim_z=1)
    angle = 2 * np.pi * kalman_frequency / sampling_rate
    kalman.F = kalman_A * np.array([[np.cos(angle), - np.sin(angle)],
                                    [np.sin(angle), np.cos(angle)]])  # state transition matrix
    kalman.H = np.array([[1., 0.]])  # Measurement function
    kalman.Q = (kalman_sigma ** 2) * np.eye(2)
    kalman.R = (kalman_delta ** 2) * np.eye(1)
    return kalman


def test_kalman_filter(kalman_A: float, kalman_frequency: float, kalman_sigma: float, kalman_delta: float,
                       sampling_rate: float, dataset: NPWrapperDataset,
                       dataset_type: str, dataset_name: str, frequency: tp.Optional[float],
                       filter_band: tp.Optional[tp.Union[float, float]], noise_coeff: tp.Optional[float],
                       model_name: tp.Optional[str], save: bool = True):
    """
    Runs a single test for a specific hyperparameters of the Kalman filter, calculates metrics and can save results

    Parameters
    ----------
    kalman_A : float
                Alpha in the state transition matrix which makes process stationary. Should be in (0, 1) range.
                Typically, is close to 1
    kalman_frequency : float
                Central frequency for the state-space process
    kalman_sigma : float
                Standard deviation for the process noise matrix
    kalman_delta : float
                Standard deviation for the measurement noise matrix
    sampling_rate : float
                Sampling rate of the data
    dataset : NPWrapperDataset
                Dataset instance on which function will test Kalman filter
    dataset_type : str
                Type of the dataset. Will be only used for writing to json with results as a hint
    dataset_name : str
                Name of the dataset. One of ['sines_white', 'sines_pink', 'state_space_white', 'state_space_pink',
                'filtered_pink', 'multiperson_real_data']. Will be only used for writing to json with results as a hint
    frequency : float
                Central frequency for the dataset. Will be only used for writing to json with results as a hint.
                Suggested to use None if dataset is filtered_pink (relies on filter_band, not central frequency during
                generation) or multiperson_real_data (ground truth central frequency is unknown)
    filter_band : tuple (freq_low, freq_high)
                Use only for filtered_pink dataset. This dataset is generated by adding a sample of pink noise to
                another sample of pink noise filtered in the filter_band. For others use None.
                Will be only used for writing to json with results as a hint
    noise_coeff : float
                Noise coefficient by which a sample of noise is amplified. Used to calculate Noise Level.
                Use None for multiperson_real_data as here the noise comes from the data rather that being added during
                generation. Will be only used for writing to json with results as a hint
    model_name : str
                Name of the model to write to appropriate model folder in the results directory. Your path to saved
                results will be results/test_results/model_name/YYYY-MM-DD/00-00-00
    save : bool
                Whether to save the results or not

    Returns
    -------
    detailed_errors_dict : dict
                Dictionary which has lists of error for appropriate metrics
                ['correlation', 'plv', 'circstd', 'circstd_degrees']
    predictions : dict
                Dictionary with predictions for the dataset (observations filtered with a Kalman filter). It has
                keys 'complex_pred', 'complex_gt' for aligned filtration results and ground truth respectively. It also
                has keys 'envelope_pred' and 'envelope_gt' for their respective envelopes
    """
    assert dataset_name in ['sines_white', 'sines_pink', 'state_space_white', 'state_space_pink', 'filtered_pink',
                            'multiperson_real_data']
    ds_size = dataset.get_size()
    output_size = dataset.output_size
    filtered_series = []
    gt_series = []
    for idx in tqdm(range(ds_size)):
        kalman = init_kalman(kalman_A=kalman_A, kalman_frequency=kalman_frequency,
                             kalman_sigma=kalman_sigma, kalman_delta=kalman_delta,
                             sampling_rate=sampling_rate)
        datapoint_X, gt_y = dataset.get_datapoint(idx)
        assert len(datapoint_X.shape) == 2
        assert len(gt_y.shape) == 1
        time_size = datapoint_X.shape[0]
        filtered_X = np.zeros((time_size, 2, 1))
        for idx2 in range(time_size):
            kalman.predict()
            kalman.update(datapoint_X[idx2, :])
            filtered_X[idx2, ...] = kalman.x
        filtered_X = filtered_X[:, 0, 0] + 1.j * filtered_X[:, 1, 0]
        filtered_X = filtered_X[-output_size:]
        filtered_series.append(filtered_X)
        gt_series.append(gt_y)
    filtered_series = np.array(filtered_series)
    gt_series = np.array(gt_series)
    assert filtered_series.shape == gt_series.shape

    detailed_errors_dict, aggregated, predictions = calculate_metrics(complex_pred=filtered_series,
                                                                      complex_gt=gt_series)

    print(dataset_name)
    for metric, agg in aggregated.items():
        print(metric, agg)
    print('\n')

    if save:

        save_results(detailed_errors_dict=detailed_errors_dict,
                     predictions=predictions, dataset_type=dataset_type, dataset_name=dataset_name,
                     frequency=frequency, filter_band=filter_band, sampling_rate=sampling_rate,
                     noise_coeff=noise_coeff, model_name=model_name,
                     model_params={'kalman_A': kalman_A, 'kalman_frequency': kalman_frequency,
                                   'kalman_sigma': kalman_sigma, 'kalman_delta': kalman_delta})

    return detailed_errors_dict, predictions


def optuna_kalman_objective(trial: Study, metric_name: str, sampling_rate: float,
                            dataset: NPWrapperDataset,
                            dataset_type: str, dataset_name: str):
    """
    Objective function for the optuna framework to fit Kalman filter hyperparameters

    Parameters
    ----------
    trial: Study
                Optuna Study instance. Typically, is created with optuna.create_study(direction=direction), where
                direction is maximize or minimize.
    metric_name : str
                One of ['correlation', 'plv', 'circstd', 'circstd_degrees']. Choose metric according to which Kalman
                filter's hyperparameters will be optimized
    sampling_rate : float
                Sampling rate of the data
    dataset : NPWrapperDataset
                Dataset instance on which function will test Kalman filter
    dataset_type : str
                Type of the dataset
    dataset_name : str
                Name of the dataset. One of ['sines_white', 'sines_pink', 'state_space_white', 'state_space_pink',
                'filtered_pink', 'multiperson_real_data']

    Returns
    -------
    mean_error : float
                Mean value of the metric_name over dataset
    """
    kalman_A = trial.suggest_float('kalman_A', 0.98, 1.01)
    kalman_frequency = trial.suggest_float('kalman_frequency', 9, 12)
    kalman_sigma = trial.suggest_float('kalman_sigma', 0.01, 0.2)
    kalman_delta = trial.suggest_float('kalman_delta', 1, 2.5)
    detailed_errors_dict, predictions = test_kalman_filter(kalman_A=kalman_A,
                                                           kalman_frequency=kalman_frequency,
                                                           kalman_sigma=kalman_sigma,
                                                           kalman_delta=kalman_delta,
                                                           sampling_rate=sampling_rate,
                                                           dataset=dataset,
                                                           dataset_type=dataset_type,
                                                           dataset_name=dataset_name,
                                                           frequency=None, filter_band=None,
                                                           noise_coeff=None, model_name=None,
                                                           save=False)

    return np.mean(detailed_errors_dict[metric_name])


def optimize_and_test_kalman_baseline(dataset_name: str, dataset_path: str):
    """
    General function for tuning Kalman filter for dataset_name on the train split, testing on the generated test split
    and saving results.

    Parameters
    ----------
    dataset_name : str
                Name of the dataset. One of ['sines_white', 'sines_pink', 'state_space_white', 'state_space_pink',
                'filtered_pink', 'multiperson_real_data']
    dataset_path : str
                Path to the train_split of the dataset in case of 'sines_white', 'sines_pink', 'filtered_pink',
                'state_space_white', 'state_space_pink' (test split will be generated) or to the whole real dataset
                (will be split into train and test)
    """

    if dataset_name in ['sines_white', 'sines_pink', 'filtered_pink', 'state_space_white', 'state_space_pink']:

        optimize_and_test_synthetic(model_name='KalmanFilter',
                                    optuna_objective=optuna_kalman_objective,
                                    optuna_parameters_restructurer=None,
                                    dataset_name=dataset_name, train_dataset_path=dataset_path,
                                    add_X_dimension=True, test_function=test_kalman_filter,
                                    n_trials=100, train_amount_of_observations=750)

    elif dataset_name == 'multiperson_real_data':

        optimize_and_test_real(model_name='KalmanFilter',
                               optuna_objective=optuna_kalman_objective,
                               optuna_parameters_restructurer=None,
                               dataset_path=dataset_path,
                               add_X_dimension=True, test_function=test_kalman_filter,
                               n_trials=100)

    else:
        raise ValueError('dataset_name is not recognized. Please choose one from the following list of synthetic '
                         'datasets: [sines_white, sines_pink, filtered_pink, state_space_white, state_space_pink] or '
                         'a multiperson_real_data for real dataset.')


if __name__ == '__main__':
    make_deterministic(seed=0)
    parser = argparse.ArgumentParser(description='Optimizing hyperparameters and testing Kalman Filter model '
                                                 'on a synthetic generated or real multi-person dataset.')
    parser.add_argument('-n', '--dataset_name', type=str,
                        choices=['sines_white', 'sines_pink', 'state_space_white', 'state_space_pink', 'filtered_pink',
                                 'multiperson_real_data'],
                        help='Name of the required dataset for generation', required=True)
    parser.add_argument('-t', '--dataset_path', type=str,
                        help='Path to the dataset from the current working directory. '
                             'Will be used for finding hyperparameters on synthetic dataset and '
                             'for optimizing hyperparameters and testing for multi-person real dataset', required=True)
    args = parser.parse_args()
    optimize_and_test_kalman_baseline(dataset_name=args.dataset_name, dataset_path=args.dataset_path)
